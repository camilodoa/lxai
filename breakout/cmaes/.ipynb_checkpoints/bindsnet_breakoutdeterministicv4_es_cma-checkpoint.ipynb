{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torch) (1.20.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torchvision) (1.20.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: cma in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting BindsNET@ git+https://github.com/BindsNET/bindsnet.git@ead55217e05ba4c6ef27f45ff5dc7d61b4abaa13\n",
      "  Using cached BindsNET-0.2.9-py3-none-any.whl\n",
      "Collecting gym@ git+https://github.com/openai/gym.git@a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022\n",
      "  Using cached gym-0.18.0-py3-none-any.whl\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: atari-py==0.2.6 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 2)) (0.2.6)\n",
      "Requirement already satisfied: attrs==20.3.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 3)) (20.3.0)\n",
      "Requirement already satisfied: box2d-py==2.3.8 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 5)) (2.3.8)\n",
      "Requirement already satisfied: cffi==1.14.5 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 6)) (1.14.5)\n",
      "Requirement already satisfied: cloudpickle==1.2.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 8)) (0.10.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 9)) (0.29.23)\n",
      "Requirement already satisfied: decorator==4.4.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 10)) (4.4.2)\n",
      "Requirement already satisfied: distlib==0.3.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 11)) (0.3.1)\n",
      "Requirement already satisfied: distro==1.5.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 12)) (1.5.0)\n",
      "Requirement already satisfied: fasteners==0.16 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 13)) (0.16)\n",
      "Requirement already satisfied: filelock==3.0.12 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 14)) (3.0.12)\n",
      "Requirement already satisfied: glfw==2.1.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 15)) (2.1.0)\n",
      "Requirement already satisfied: imageio==2.9.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 17)) (2.9.0)\n",
      "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 18)) (1.1.1)\n",
      "Requirement already satisfied: joblib==1.0.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 19)) (1.0.1)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 20)) (1.3.1)\n",
      "Requirement already satisfied: matplotlib==3.4.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 21)) (3.4.1)\n",
      "Requirement already satisfied: mujoco-py==2.0.2.13 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 22)) (2.0.2.13)\n",
      "Requirement already satisfied: networkx==2.5.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 23)) (2.5.1)\n",
      "Requirement already satisfied: numpy==1.20.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 24)) (1.20.2)\n",
      "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 25)) (4.5.1.48)\n",
      "Requirement already satisfied: packaging==20.9 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 26)) (20.9)\n",
      "Requirement already satisfied: pandas==1.2.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 27)) (1.2.4)\n",
      "Requirement already satisfied: pbr==5.6.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 28)) (5.6.0)\n",
      "Requirement already satisfied: Pillow==8.2.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 29)) (8.2.0)\n",
      "Requirement already satisfied: pluggy==0.13.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 30)) (0.13.1)\n",
      "Requirement already satisfied: protobuf==3.15.8 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 31)) (3.15.8)\n",
      "Requirement already satisfied: py==1.10.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 32)) (1.10.0)\n",
      "Requirement already satisfied: pybullet==3.1.6 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 33)) (3.1.6)\n",
      "Requirement already satisfied: pycparser==2.20 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 34)) (2.20)\n",
      "Requirement already satisfied: pyglet==1.5.11 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 35)) (1.5.11)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 36)) (2.4.7)\n",
      "Requirement already satisfied: pytest==6.2.3 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 37)) (6.2.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 38)) (2.8.1)\n",
      "Requirement already satisfied: pytz==2021.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 39)) (2021.1)\n",
      "Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 40)) (1.1.1)\n",
      "Requirement already satisfied: scikit-build==0.11.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 41)) (0.11.1)\n",
      "Requirement already satisfied: scikit-image==0.18.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 42)) (0.18.1)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 43)) (0.24.1)\n",
      "Requirement already satisfied: scipy==1.6.3 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 44)) (1.6.3)\n",
      "Requirement already satisfied: six==1.15.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 45)) (1.15.0)\n",
      "Requirement already satisfied: stevedore==3.3.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 46)) (3.3.0)\n",
      "Requirement already satisfied: tensorboardX==2.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 47)) (2.2)\n",
      "Requirement already satisfied: threadpoolctl==2.1.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 48)) (2.1.0)\n",
      "Requirement already satisfied: tifffile==2021.4.8 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 49)) (2021.4.8)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 50)) (0.10.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 51)) (1.8.1)\n",
      "Requirement already satisfied: torchvision==0.9.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 52)) (0.9.1)\n",
      "Requirement already satisfied: tqdm==4.60.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 53)) (4.60.0)\n",
      "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 54)) (3.7.4.3)\n",
      "Requirement already satisfied: virtualenv==20.4.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 55)) (20.4.4)\n",
      "Requirement already satisfied: virtualenv-clone==0.5.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 56)) (0.5.4)\n",
      "Requirement already satisfied: virtualenvwrapper==4.8.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 57)) (4.8.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel>=0.29.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from scikit-build==0.11.1->-r ../../requirements.txt (line 41)) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=28.0.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from scikit-build==0.11.1->-r ../../requirements.txt (line 41)) (54.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install cma\n",
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import cma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import codecs\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count() 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "print(\"torch.cuda.device_count()\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', ['epochs', 'batch_size', 'lr', 'cuda', 'seed', 'log_interval', 'env', 'fitness_episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(batch_size=1000, epochs=30, lr=0.001, cuda=False, seed=0, log_interval=10, env='BreakoutDeterministic-v4', fitness_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(x):\n",
    "  \"\"\"\n",
    "  Returns ranks in [0, len(x))\n",
    "  Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "  (https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py)\n",
    "  \"\"\"\n",
    "  assert x.ndim == 1\n",
    "  ranks = np.empty(len(x), dtype=int)\n",
    "  ranks[x.argsort()] = np.arange(len(x))\n",
    "  return ranks\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "  \"\"\"\n",
    "  https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py\n",
    "  \"\"\"\n",
    "  y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "  y /= (x.size - 1)\n",
    "  y -= .5\n",
    "  return y\n",
    "\n",
    "def compute_weight_decay(weight_decay, model_param_list):\n",
    "  model_param_grid = np.array(model_param_list)\n",
    "  return - weight_decay * np.mean(model_param_grid * model_param_grid, axis=1)\n",
    "\n",
    "class CMAES:\n",
    "  '''CMA-ES wrapper.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.10,       # initial standard deviation\n",
    "               popsize=255):          # population size\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.popsize = popsize\n",
    "\n",
    "    self.solutions = None\n",
    "\n",
    "    import cma\n",
    "    self.es = cma.CMAEvolutionStrategy( self.num_params * [0],\n",
    "                                        self.sigma_init,\n",
    "                                        {'popsize': self.popsize})\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.es.result[6]\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    self.solutions = np.array(self.es.ask())\n",
    "    return self.solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    reward_table = reward_table_result\n",
    "    self.es.tell(self.solutions, (-reward_table).tolist()) # convert minimizer to maximizer.\n",
    "\n",
    "  def done(self):\n",
    "    return self.es.stop()\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.es.result[5] # mean solution, presumably better with noise\n",
    "  \n",
    "  def best_param(self):\n",
    "    return self.es.result[0] # best evaluated solution\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    r = self.es.result\n",
    "    return (r[0], -r[1], -r[1], r[6])\n",
    "\n",
    "class SimpleES:\n",
    "  '''Simple Evolution Strategies.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.10,       # initial standard deviation\n",
    "               sigma_alpha=0.20,      # learning rate for standard deviation\n",
    "               sigma_decay=0.999,     # anneal standard deviation\n",
    "               sigma_limit=0.01,      # stop annealing if less than this\n",
    "               popsize=255,           # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               done_threshold=1e-6,   # threshold when we say we are done\n",
    "               average_baseline=True, # set baseline to average of batch\n",
    "               forget_best=True):     # only use the best from latest generation\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.sigma_alpha = sigma_alpha\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.popsize = popsize\n",
    "    self.average_baseline = average_baseline\n",
    "    if self.average_baseline:\n",
    "      assert (self.popsize & 2), \"Population size must be even\"\n",
    "      self.batch_size = int(self.popsize / 2)\n",
    "    else:\n",
    "      assert (self.popsize & 1), \"Population size must be odd\"\n",
    "      self.batch_size = int((self.popsize - 1) / 2)\n",
    "    self.elite_ratio = elite_ratio\n",
    "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
    "    self.forget_best = forget_best\n",
    "    self.batch_reward = np.zeros(self.batch_size * 2)\n",
    "    self.mu = np.zeros(self.num_params)\n",
    "    self.sigma = np.ones(self.num_params) * self.sigma_init\n",
    "    self.curr_best_mu = np.zeros(self.num_params)\n",
    "    self.best_mu = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_interation = True\n",
    "    self.done_threshold = done_threshold\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.sigma\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    self.epsilon = np.random.randn(self.batch_size, self.num_params) * self.sigma.reshape(1, self.num_params)\n",
    "    self.epsilon_full = np.concatenate([self.epsilon, - self.epsilon])\n",
    "    if self.average_baseline:\n",
    "      epsilon = self.epsilon_full\n",
    "    else:\n",
    "      # first population is mu, then positive epsilon, then negative epsilon\n",
    "      epsilon = np.concatenate([np.zeros((1, self.num_params)), self.epsilon_full])\n",
    "    solutions = self.mu.reshape(1, self.num_params) + epsilon\n",
    "    return solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "\n",
    "    reward_table = reward_table_result\n",
    "\n",
    "    reward_offset = 1\n",
    "    if self.average_baseline:\n",
    "      b = np.mean(reward_table)\n",
    "      reward_offset = 0\n",
    "    else:\n",
    "      b = reward_table[0] # baseline\n",
    "      \n",
    "    reward = reward_table[reward_offset:]\n",
    "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
    "\n",
    "    best_reward = reward[idx[0]]\n",
    "    if (best_reward > b or self.average_baseline):\n",
    "      best_mu = self.mu + self.epsilon_full[idx[0]]\n",
    "      best_reward = reward[idx[0]]\n",
    "    else:\n",
    "      best_mu = self.mu\n",
    "      best_reward = b\n",
    "\n",
    "    self.curr_best_reward = best_reward\n",
    "    self.curr_best_mu = best_mu\n",
    "\n",
    "    if self.first_interation:\n",
    "      self.first_interation = False\n",
    "      self.best_reward = self.curr_best_reward\n",
    "      self.best_mu = best_mu\n",
    "    else:\n",
    "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "        self.best_mu = best_mu\n",
    "        self.best_reward = self.curr_best_reward\n",
    "\n",
    "    # adaptive sigma\n",
    "    # normalization\n",
    "    stdev_reward = reward.std()\n",
    "    epsilon = self.epsilon\n",
    "    sigma = self.sigma\n",
    "    S = ((epsilon * epsilon - (sigma * sigma).reshape(1, self.num_params)) / sigma.reshape(1, self.num_params))\n",
    "    reward_avg = (reward[:self.batch_size] + reward[self.batch_size:]) / 2.0\n",
    "    rS = reward_avg - b\n",
    "    delta_sigma = (np.dot(rS, S)) / (2 * self.batch_size * stdev_reward)\n",
    "\n",
    "    # move mean to the average of the best idx means\n",
    "    self.mu += self.epsilon_full[idx].mean(axis=0)\n",
    "\n",
    "    # adjust sigma according to the adaptive sigma calculation\n",
    "    change_sigma = self.sigma_alpha * delta_sigma\n",
    "    change_sigma = np.minimum(change_sigma, self.sigma)\n",
    "    change_sigma = np.maximum(change_sigma, - 0.5 * self.sigma)\n",
    "    self.sigma += change_sigma\n",
    "    self.sigma[self.sigma > self.sigma_limit] *= self.sigma_decay\n",
    "\n",
    "  def done(self):\n",
    "    return (self.rms_stdev() < self.done_threshold)\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.curr_best_mu\n",
    "  \n",
    "  def best_param(self):\n",
    "    return self.best_mu\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)\n",
    "\n",
    "class SimpleGA:\n",
    "  '''Simple Genetic Algorithm.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.1,        # initial standard deviation\n",
    "               sigma_decay=0.999,     # anneal standard deviation\n",
    "               sigma_limit=0.01,      # stop annealing if less than this\n",
    "               popsize=255,           # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               forget_best=False,     # forget the historical best elites\n",
    "               done_threshold=1e-6):  # threshold when we say we are done\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.popsize = popsize\n",
    "\n",
    "    self.elite_ratio = elite_ratio\n",
    "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
    "\n",
    "    self.sigma = self.sigma_init\n",
    "    self.elite_params = np.zeros((self.elite_popsize, self.num_params))\n",
    "    self.elite_rewards = np.zeros(self.elite_popsize)\n",
    "    self.best_param = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_iteration = True\n",
    "    self.forget_best = forget_best\n",
    "    self.done_threshold = done_threshold\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    return self.sigma # same sigma for all parameters.\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    self.epsilon = np.random.randn(self.popsize, self.num_params) * self.sigma\n",
    "    solutions = []\n",
    "    \n",
    "    def mate(a, b):\n",
    "      c = np.copy(a)\n",
    "      idx = np.where(np.random.rand((c.size)) > 0.5)\n",
    "      c[idx] = b[idx]\n",
    "      return c\n",
    "    \n",
    "    elite_range = range(self.elite_popsize)\n",
    "    for i in range(self.popsize):\n",
    "      idx_a = np.random.choice(elite_range)\n",
    "      idx_b = np.random.choice(elite_range)\n",
    "      child_params = mate(self.elite_params[idx_a], self.elite_params[idx_b])\n",
    "      solutions.append(child_params + self.epsilon[i])\n",
    "\n",
    "    solutions = np.array(solutions)\n",
    "    self.solutions = solutions\n",
    "\n",
    "    return solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "    \n",
    "    if (not self.forget_best or self.first_iteration):\n",
    "      reward = reward_table_result\n",
    "      solution = self.solutions\n",
    "    else:\n",
    "      reward = np.concatenate([reward_table_result, self.elite_rewards])\n",
    "      solution = np.concatenate([self.solutions, self.elite_params])\n",
    "\n",
    "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
    "\n",
    "    self.elite_rewards = reward[idx]\n",
    "    self.elite_params = solution[idx]\n",
    "\n",
    "    self.curr_best_reward = self.elite_rewards[0]\n",
    "    \n",
    "    if self.first_iteration or (self.curr_best_reward > self.best_reward):\n",
    "      self.first_iteration = False\n",
    "      self.best_reward = self.elite_rewards[0]\n",
    "      self.best_param = np.copy(self.elite_params[0])\n",
    "\n",
    "    if (self.sigma > self.sigma_limit):\n",
    "      self.sigma *= self.sigma_decay\n",
    "\n",
    "  def done(self):\n",
    "    return (self.rms_stdev() < self.done_threshold)\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.elite_params[0]\n",
    "\n",
    "  def best_param(self):\n",
    "    return self.best_param\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_param, self.best_reward, self.curr_best_reward, self.sigma)\n",
    "\n",
    "class OpenES:\n",
    "  ''' Basic Version of OpenAI Evolution Strategies.'''\n",
    "  def __init__(self, num_params,             # number of model parameters\n",
    "               sigma_init=0.1,               # initial standard deviation\n",
    "               sigma_decay=0.999,            # anneal standard deviation\n",
    "               sigma_limit=0.01,             # stop annealing if less than this\n",
    "               learning_rate=0.001,          # learning rate for standard deviation\n",
    "               learning_rate_decay = 0.9999, # annealing the learning rate\n",
    "               learning_rate_limit = 0.001,  # stop annealing learning rate\n",
    "               popsize=255,                  # population size\n",
    "               antithetic=False,             # whether to use antithetic sampling\n",
    "               forget_best=True):           # forget historical best\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma = sigma_init\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.learning_rate = learning_rate\n",
    "    self.learning_rate_decay = learning_rate_decay\n",
    "    self.learning_rate_limit = learning_rate_limit\n",
    "    self.popsize = popsize\n",
    "    self.antithetic = antithetic\n",
    "    if self.antithetic:\n",
    "      assert (self.popsize & 2), \"Population size must be even\"\n",
    "      self.half_popsize = int(self.popsize / 2)\n",
    "\n",
    "    self.reward = np.zeros(self.popsize)\n",
    "    self.mu = np.zeros(self.num_params)\n",
    "    self.best_mu = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_interation = True\n",
    "    self.forget_best = forget_best\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.sigma\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    if self.antithetic:\n",
    "      self.epsilon_half = np.random.randn(self.half_popsize, self.num_params)\n",
    "      self.epsilon = np.concatenate([self.epsilon_half, - self.epsilon_half])\n",
    "    else:\n",
    "      self.epsilon = np.random.randn(self.popsize, self.num_params)\n",
    "\n",
    "    self.solutions = self.mu.reshape(1, self.num_params) + self.epsilon * self.sigma\n",
    "\n",
    "    return self.solutions\n",
    "\n",
    "  def tell(self, reward):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "\n",
    "    idx = np.argsort(reward)[::-1]\n",
    "\n",
    "    best_reward = reward[idx[0]]\n",
    "    best_mu = self.solutions[idx[0]]\n",
    "\n",
    "    self.curr_best_reward = best_reward\n",
    "    self.curr_best_mu = best_mu\n",
    "\n",
    "    if self.first_interation:\n",
    "      self.first_interation = False\n",
    "      self.best_reward = self.curr_best_reward\n",
    "      self.best_mu = best_mu\n",
    "    else:\n",
    "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "        self.best_mu = best_mu\n",
    "        self.best_reward = self.curr_best_reward\n",
    "\n",
    "    # main bit:\n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    normalized_reward = (reward - np.mean(reward)) / np.std(reward)\n",
    "    self.mu += self.learning_rate/(self.popsize*self.sigma)*np.dot(self.epsilon.T, normalized_reward)\n",
    "\n",
    "    # adjust sigma according to the adaptive sigma calculation\n",
    "    if (self.sigma > self.sigma_limit):\n",
    "      self.sigma *= self.sigma_decay\n",
    "\n",
    "    if (self.learning_rate > self.learning_rate_limit):\n",
    "      self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "  def done(self):\n",
    "    return False\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.curr_best_mu\n",
    "\n",
    "  def best_param(self):\n",
    "    return self.best_mu\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"DQN Network\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        self.num_filter1 = 8\n",
    "        self.num_filter2 = 16\n",
    "        self.num_padding = 2\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, self.num_filter1, 5, padding=self.num_padding)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(self.num_filter1, self.num_filter2, 5, padding=self.num_padding)\n",
    "        \n",
    "        self.fc = nn.Linear(6400, 4)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns a Q_value\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(6400) \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        \"\"\"Agent class\n",
    "        \"\"\"\n",
    "        self.net = model\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.net.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"torch.Variable syntax helper\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray) -> int:\n",
    "        \"\"\"Returns an action\n",
    "        \"\"\"\n",
    "        self.net.train(mode=False)\n",
    "        scores = self.get_Q(np.array([states]))\n",
    "        argmax = torch.argmax(scores)\n",
    "        return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        \"\"\"Returns `Q-value`\n",
    "        \"\"\"\n",
    "        states = self._to_variable(states)\n",
    "        self.net.train(mode=False)\n",
    "\n",
    "        return self.net(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        \"\"\"\n",
    "        self.net.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        \"\"\"Agent class\n",
    "        \"\"\"\n",
    "        self.net = model\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.net.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"torch.Variable syntax helper\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray) -> int:\n",
    "        \"\"\"Returns an action\n",
    "        \"\"\"\n",
    "        if np.random.rand() < 0.1:\n",
    "            return np.random.choice(4)\n",
    "        else:\n",
    "            self.net.train(mode=False)\n",
    "            scores = self.get_Q(np.array([states]))\n",
    "            argmax = torch.argmax(scores)\n",
    "            return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        \"\"\"Returns `Q-value`\n",
    "        \"\"\"\n",
    "        states = self._to_variable(states)\n",
    "        self.net.train(mode=False)\n",
    "\n",
    "        return self.net(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        \"\"\"\n",
    "        self.net.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_reward(reward):\n",
    "    \"\"\"Clip reward so that it's in [-1, 1]\n",
    "    \"\"\"\n",
    "    if reward < -1:\n",
    "        reward = -1\n",
    "    elif reward > 1:\n",
    "        reward = 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(states: np.ndarray):\n",
    "    \"\"\"Preprocesses gym state\n",
    "    \"\"\"\n",
    "    # Crop\n",
    "    states = states[34:194, 0:160, :]\n",
    "\n",
    "    # Convert to grayscale\n",
    "    states = cv2.cvtColor(states, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Subsample to 80x80\n",
    "    states = cv2.resize(states, (80, 80))\n",
    "    states = cv2.threshold(states, 0, 1, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    states = states.reshape(1, states.shape[0], states.shape[1])\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env: gym.Env, agent: Agent) -> int:\n",
    "    \"\"\"Play an episode\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    s = preprocess(s)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        a = agent.get_action(s)\n",
    "        s2, r, done, info = env.step(a)\n",
    "        env.render()\n",
    "\n",
    "        # Preprocessing step\n",
    "        s2 = preprocess(s2)\n",
    "        r = clip_reward(r)\n",
    "\n",
    "        total_reward += r\n",
    "        \n",
    "        s = s2\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPOPULATION = 101\n",
    "weight_decay_coef = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models = []\n",
    "for i in range(NPOPULATION):\n",
    "  model = Net()\n",
    "  if args.cuda:\n",
    "    model.cuda()\n",
    "  model.eval()\n",
    "  models.append(model)\n",
    "'''\n",
    "\n",
    "model = Net()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "orig_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29028\n"
     ]
    }
   ],
   "source": [
    "# get init params\n",
    "orig_params = []\n",
    "model_shapes = []\n",
    "for param in orig_model.parameters():\n",
    "    p = param.data.cpu().numpy()\n",
    "    model_shapes.append(p.shape)\n",
    "    orig_params.append(p.flatten())\n",
    "orig_params_flat = np.concatenate(orig_params)\n",
    "NPARAMS = len(orig_params_flat)\n",
    "print(NPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(flat_param, model, model_shapes):\n",
    "    idx = 0\n",
    "    i = 0\n",
    "    for param in model.parameters():\n",
    "        delta = np.product(model_shapes[i])\n",
    "        block = flat_param[idx:idx+delta]\n",
    "        block = np.reshape(block, model_shapes[i])\n",
    "        i += 1\n",
    "        idx += delta\n",
    "        block_data = torch.from_numpy(block).float()\n",
    "        if args.cuda:\n",
    "          block_data = block_data.cuda()\n",
    "        param.data = block_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, print_mode=True):\n",
    "    agent = Agent(model)\n",
    "    total_reward = 0\n",
    "    for i in range(args.fitness_episodes):\n",
    "        reward = play_episode(env, agent)\n",
    "        total_reward += reward\n",
    "        \n",
    "    if print_mode:\n",
    "        print('\\nAverage reward: {:.4f}'.format(total_reward/args.fitness_episodes))\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50_w,101)-aCMA-ES (mu_w=27.2,w_1=8%) in dimension 29028 (seed=571837, Thu Oct 28 18:42:51 2021)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "es = SimpleES(NPARAMS,\n",
    "              popsize=NPOPULATION,\n",
    "              sigma_init=0.01,\n",
    "              sigma_decay=0.999,\n",
    "              sigma_alpha=0.2,\n",
    "              sigma_limit=0.001,\n",
    "              elite_ratio=0.1,\n",
    "              average_baseline=False,\n",
    "              forget_best=True\n",
    "             )\n",
    "\"\"\"\n",
    "es = CMAES(NPARAMS, sigma_init=0.01, popsize=NPOPULATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-2e1dd954fcb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbest_raw_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-7eddcbd94076>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, env, print_mode)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-30b8883c43f4>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Preprocessing step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(args.env)\n",
    "try:\n",
    "    best_valid_acc = 0\n",
    "    training_log = []\n",
    "    for epoch in range(1, args.epochs):\n",
    "\n",
    "        # train loop\n",
    "        model.eval()\n",
    "        solutions = es.ask()\n",
    "        reward = np.zeros(es.popsize)\n",
    "\n",
    "        for i in range(es.popsize):\n",
    "            update_model(solutions[i], model, model_shapes)\n",
    "            reward[i] = evaluate(model, env, print_mode=False)\n",
    "\n",
    "        best_raw_reward = reward.max()\n",
    "        es.tell(reward)\n",
    "        result = es.result()\n",
    "\n",
    "        curr_solution = es.current_param()\n",
    "        update_model(curr_solution, model, model_shapes)\n",
    "        solution_reward = evaluate(model, env, print_mode=True)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
