{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (1.9.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: torch==1.9.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torchvision) (1.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torchvision) (1.21.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torchvision) (8.3.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from torch==1.9.1->torchvision) (3.10.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting cma\n",
      "  Downloading cma-3.1.0-py2.py3-none-any.whl (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cma\n",
      "Successfully installed cma-3.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import codecs\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count() 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "print(\"torch.cuda.device_count()\", torch.cuda.device_count())\n",
    "# print(\"torch.cuda.current_device()\", torch.cuda.current_device())\n",
    "# torch.cuda.set_device(3)\n",
    "# print(\"torch.cuda.current_device()\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(x):\n",
    "  \"\"\"\n",
    "  Returns ranks in [0, len(x))\n",
    "  Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "  (https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py)\n",
    "  \"\"\"\n",
    "  assert x.ndim == 1\n",
    "  ranks = np.empty(len(x), dtype=int)\n",
    "  ranks[x.argsort()] = np.arange(len(x))\n",
    "  return ranks\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "  \"\"\"\n",
    "  https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py\n",
    "  \"\"\"\n",
    "  y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "  y /= (x.size - 1)\n",
    "  y -= .5\n",
    "  return y\n",
    "\n",
    "def compute_weight_decay(weight_decay, model_param_list):\n",
    "  model_param_grid = np.array(model_param_list)\n",
    "  return - weight_decay * np.mean(model_param_grid * model_param_grid, axis=1)\n",
    "\n",
    "class CMAES:\n",
    "  '''CMA-ES wrapper.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.10,       # initial standard deviation\n",
    "               popsize=255):          # population size\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.popsize = popsize\n",
    "\n",
    "    self.solutions = None\n",
    "\n",
    "    import cma\n",
    "    self.es = cma.CMAEvolutionStrategy( self.num_params * [0],\n",
    "                                        self.sigma_init,\n",
    "                                        {'popsize': self.popsize})\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.es.result[6]\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    self.solutions = np.array(self.es.ask())\n",
    "    return self.solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    reward_table = reward_table_result\n",
    "    self.es.tell(self.solutions, (-reward_table).tolist()) # convert minimizer to maximizer.\n",
    "\n",
    "  def done(self):\n",
    "    return self.es.stop()\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.es.result[5] # mean solution, presumably better with noise\n",
    "  \n",
    "  def best_param(self):\n",
    "    return self.es.result[0] # best evaluated solution\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    r = self.es.result\n",
    "    return (r[0], -r[1], -r[1], r[6])\n",
    "\n",
    "class SimpleES:\n",
    "  '''Simple Evolution Strategies.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.10,       # initial standard deviation\n",
    "               sigma_alpha=0.20,      # learning rate for standard deviation\n",
    "               sigma_decay=0.999,     # anneal standard deviation\n",
    "               sigma_limit=0.01,      # stop annealing if less than this\n",
    "               popsize=255,           # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               done_threshold=1e-6,   # threshold when we say we are done\n",
    "               average_baseline=True, # set baseline to average of batch\n",
    "               forget_best=True):     # only use the best from latest generation\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.sigma_alpha = sigma_alpha\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.popsize = popsize\n",
    "    self.average_baseline = average_baseline\n",
    "    if self.average_baseline:\n",
    "      assert (self.popsize & 2), \"Population size must be even\"\n",
    "      self.batch_size = int(self.popsize / 2)\n",
    "    else:\n",
    "      assert (self.popsize & 1), \"Population size must be odd\"\n",
    "      self.batch_size = int((self.popsize - 1) / 2)\n",
    "    self.elite_ratio = elite_ratio\n",
    "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
    "    self.forget_best = forget_best\n",
    "    self.batch_reward = np.zeros(self.batch_size * 2)\n",
    "    self.mu = np.zeros(self.num_params)\n",
    "    self.sigma = np.ones(self.num_params) * self.sigma_init\n",
    "    self.curr_best_mu = np.zeros(self.num_params)\n",
    "    self.best_mu = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_interation = True\n",
    "    self.done_threshold = done_threshold\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.sigma\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    self.epsilon = np.random.randn(self.batch_size, self.num_params) * self.sigma.reshape(1, self.num_params)\n",
    "    self.epsilon_full = np.concatenate([self.epsilon, - self.epsilon])\n",
    "    if self.average_baseline:\n",
    "      epsilon = self.epsilon_full\n",
    "    else:\n",
    "      # first population is mu, then positive epsilon, then negative epsilon\n",
    "      epsilon = np.concatenate([np.zeros((1, self.num_params)), self.epsilon_full])\n",
    "    solutions = self.mu.reshape(1, self.num_params) + epsilon\n",
    "    return solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "\n",
    "    reward_table = reward_table_result\n",
    "\n",
    "    reward_offset = 1\n",
    "    if self.average_baseline:\n",
    "      b = np.mean(reward_table)\n",
    "      reward_offset = 0\n",
    "    else:\n",
    "      b = reward_table[0] # baseline\n",
    "      \n",
    "    reward = reward_table[reward_offset:]\n",
    "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
    "\n",
    "    best_reward = reward[idx[0]]\n",
    "    if (best_reward > b or self.average_baseline):\n",
    "      best_mu = self.mu + self.epsilon_full[idx[0]]\n",
    "      best_reward = reward[idx[0]]\n",
    "    else:\n",
    "      best_mu = self.mu\n",
    "      best_reward = b\n",
    "\n",
    "    self.curr_best_reward = best_reward\n",
    "    self.curr_best_mu = best_mu\n",
    "\n",
    "    if self.first_interation:\n",
    "      self.first_interation = False\n",
    "      self.best_reward = self.curr_best_reward\n",
    "      self.best_mu = best_mu\n",
    "    else:\n",
    "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "        self.best_mu = best_mu\n",
    "        self.best_reward = self.curr_best_reward\n",
    "\n",
    "    # adaptive sigma\n",
    "    # normalization\n",
    "    stdev_reward = reward.std()\n",
    "    epsilon = self.epsilon\n",
    "    sigma = self.sigma\n",
    "    S = ((epsilon * epsilon - (sigma * sigma).reshape(1, self.num_params)) / sigma.reshape(1, self.num_params))\n",
    "    reward_avg = (reward[:self.batch_size] + reward[self.batch_size:]) / 2.0\n",
    "    rS = reward_avg - b\n",
    "    delta_sigma = (np.dot(rS, S)) / (2 * self.batch_size * stdev_reward)\n",
    "\n",
    "    # move mean to the average of the best idx means\n",
    "    self.mu += self.epsilon_full[idx].mean(axis=0)\n",
    "\n",
    "    # adjust sigma according to the adaptive sigma calculation\n",
    "    change_sigma = self.sigma_alpha * delta_sigma\n",
    "    change_sigma = np.minimum(change_sigma, self.sigma)\n",
    "    change_sigma = np.maximum(change_sigma, - 0.5 * self.sigma)\n",
    "    self.sigma += change_sigma\n",
    "    self.sigma[self.sigma > self.sigma_limit] *= self.sigma_decay\n",
    "\n",
    "  def done(self):\n",
    "    return (self.rms_stdev() < self.done_threshold)\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.curr_best_mu\n",
    "  \n",
    "  def best_param(self):\n",
    "    return self.best_mu\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)\n",
    "\n",
    "class SimpleGA:\n",
    "  '''Simple Genetic Algorithm.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.1,        # initial standard deviation\n",
    "               sigma_decay=0.999,     # anneal standard deviation\n",
    "               sigma_limit=0.01,      # stop annealing if less than this\n",
    "               popsize=255,           # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               forget_best=False,     # forget the historical best elites\n",
    "               done_threshold=1e-6):  # threshold when we say we are done\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.popsize = popsize\n",
    "\n",
    "    self.elite_ratio = elite_ratio\n",
    "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
    "\n",
    "    self.sigma = self.sigma_init\n",
    "    self.elite_params = np.zeros((self.elite_popsize, self.num_params))\n",
    "    self.elite_rewards = np.zeros(self.elite_popsize)\n",
    "    self.best_param = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_iteration = True\n",
    "    self.forget_best = forget_best\n",
    "    self.done_threshold = done_threshold\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    return self.sigma # same sigma for all parameters.\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    self.epsilon = np.random.randn(self.popsize, self.num_params) * self.sigma\n",
    "    solutions = []\n",
    "    \n",
    "    def mate(a, b):\n",
    "      c = np.copy(a)\n",
    "      idx = np.where(np.random.rand((c.size)) > 0.5)\n",
    "      c[idx] = b[idx]\n",
    "      return c\n",
    "    \n",
    "    elite_range = range(self.elite_popsize)\n",
    "    for i in range(self.popsize):\n",
    "      idx_a = np.random.choice(elite_range)\n",
    "      idx_b = np.random.choice(elite_range)\n",
    "      child_params = mate(self.elite_params[idx_a], self.elite_params[idx_b])\n",
    "      solutions.append(child_params + self.epsilon[i])\n",
    "\n",
    "    solutions = np.array(solutions)\n",
    "    self.solutions = solutions\n",
    "\n",
    "    return solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "    \n",
    "    if (not self.forget_best or self.first_iteration):\n",
    "      reward = reward_table_result\n",
    "      solution = self.solutions\n",
    "    else:\n",
    "      reward = np.concatenate([reward_table_result, self.elite_rewards])\n",
    "      solution = np.concatenate([self.solutions, self.elite_params])\n",
    "\n",
    "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
    "\n",
    "    self.elite_rewards = reward[idx]\n",
    "    self.elite_params = solution[idx]\n",
    "\n",
    "    self.curr_best_reward = self.elite_rewards[0]\n",
    "    \n",
    "    if self.first_iteration or (self.curr_best_reward > self.best_reward):\n",
    "      self.first_iteration = False\n",
    "      self.best_reward = self.elite_rewards[0]\n",
    "      self.best_param = np.copy(self.elite_params[0])\n",
    "\n",
    "    if (self.sigma > self.sigma_limit):\n",
    "      self.sigma *= self.sigma_decay\n",
    "\n",
    "  def done(self):\n",
    "    return (self.rms_stdev() < self.done_threshold)\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.elite_params[0]\n",
    "\n",
    "  def best_param(self):\n",
    "    return self.best_param\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_param, self.best_reward, self.curr_best_reward, self.sigma)\n",
    "\n",
    "class OpenES:\n",
    "  ''' Basic Version of OpenAI Evolution Strategies.'''\n",
    "  def __init__(self, num_params,             # number of model parameters\n",
    "               sigma_init=0.1,               # initial standard deviation\n",
    "               sigma_decay=0.999,            # anneal standard deviation\n",
    "               sigma_limit=0.01,             # stop annealing if less than this\n",
    "               learning_rate=0.001,          # learning rate for standard deviation\n",
    "               learning_rate_decay = 0.9999, # annealing the learning rate\n",
    "               learning_rate_limit = 0.001,  # stop annealing learning rate\n",
    "               popsize=255,                  # population size\n",
    "               antithetic=False,             # whether to use antithetic sampling\n",
    "               forget_best=True):           # forget historical best\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma = sigma_init\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.learning_rate = learning_rate\n",
    "    self.learning_rate_decay = learning_rate_decay\n",
    "    self.learning_rate_limit = learning_rate_limit\n",
    "    self.popsize = popsize\n",
    "    self.antithetic = antithetic\n",
    "    if self.antithetic:\n",
    "      assert (self.popsize & 2), \"Population size must be even\"\n",
    "      self.half_popsize = int(self.popsize / 2)\n",
    "\n",
    "    self.reward = np.zeros(self.popsize)\n",
    "    self.mu = np.zeros(self.num_params)\n",
    "    self.best_mu = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_interation = True\n",
    "    self.forget_best = forget_best\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.sigma\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    if self.antithetic:\n",
    "      self.epsilon_half = np.random.randn(self.half_popsize, self.num_params)\n",
    "      self.epsilon = np.concatenate([self.epsilon_half, - self.epsilon_half])\n",
    "    else:\n",
    "      self.epsilon = np.random.randn(self.popsize, self.num_params)\n",
    "\n",
    "    self.solutions = self.mu.reshape(1, self.num_params) + self.epsilon * self.sigma\n",
    "\n",
    "    return self.solutions\n",
    "\n",
    "  def tell(self, reward):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "\n",
    "    idx = np.argsort(reward)[::-1]\n",
    "\n",
    "    best_reward = reward[idx[0]]\n",
    "    best_mu = self.solutions[idx[0]]\n",
    "\n",
    "    self.curr_best_reward = best_reward\n",
    "    self.curr_best_mu = best_mu\n",
    "\n",
    "    if self.first_interation:\n",
    "      self.first_interation = False\n",
    "      self.best_reward = self.curr_best_reward\n",
    "      self.best_mu = best_mu\n",
    "    else:\n",
    "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "        self.best_mu = best_mu\n",
    "        self.best_reward = self.curr_best_reward\n",
    "\n",
    "    # main bit:\n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    normalized_reward = (reward - np.mean(reward)) / np.std(reward)\n",
    "    self.mu += self.learning_rate/(self.popsize*self.sigma)*np.dot(self.epsilon.T, normalized_reward)\n",
    "\n",
    "    # adjust sigma according to the adaptive sigma calculation\n",
    "    if (self.sigma > self.sigma_limit):\n",
    "      self.sigma *= self.sigma_decay\n",
    "\n",
    "    if (self.learning_rate > self.learning_rate_limit):\n",
    "      self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "  def done(self):\n",
    "    return False\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.curr_best_mu\n",
    "\n",
    "  def best_param(self):\n",
    "    return self.best_mu\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', ['batch_size', 'test_batch_size', 'epochs', 'lr', 'cuda', 'seed', 'log_interval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(batch_size=1000, test_batch_size=1000, epochs=30, lr=0.001, cuda=False, seed=0, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "  torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST('MNIST_data', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "  batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "valid_loader = train_loader\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  datasets.MNIST('MNIST_data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "  batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.num_filter1 = 8\n",
    "    self.num_filter2 = 16\n",
    "    self.num_padding = 2\n",
    "    # input is 28x28\n",
    "    # padding=2 for same padding\n",
    "    self.conv1 = nn.Conv2d(1, self.num_filter1, 5, padding=self.num_padding)\n",
    "    # feature map size is 14*14 by pooling\n",
    "    # padding=2 for same padding\n",
    "    self.conv2 = nn.Conv2d(self.num_filter1, self.num_filter2, 5, padding=self.num_padding)\n",
    "    # feature map size is 7*7 by pooling\n",
    "    self.fc = nn.Linear(self.num_filter2*7*7, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "    x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "    x = x.view(-1, self.num_filter2*7*7)   # reshape Variable\n",
    "    x = self.fc(x)\n",
    "    return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPOPULATION = 101\n",
    "weight_decay_coef = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models = []\n",
    "for i in range(NPOPULATION):\n",
    "  model = Net()\n",
    "  if args.cuda:\n",
    "    model.cuda()\n",
    "  model.eval()\n",
    "  models.append(model)\n",
    "'''\n",
    "\n",
    "model = Net()\n",
    "if args.cuda:\n",
    "  model.cuda()\n",
    "\n",
    "orig_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11274\n"
     ]
    }
   ],
   "source": [
    "# get init params\n",
    "orig_params = []\n",
    "model_shapes = []\n",
    "for param in orig_model.parameters():\n",
    "  p = param.data.cpu().numpy()\n",
    "  model_shapes.append(p.shape)\n",
    "  orig_params.append(p.flatten())\n",
    "orig_params_flat = np.concatenate(orig_params)\n",
    "NPARAMS = len(orig_params_flat)\n",
    "print(NPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(flat_param, model, model_shapes):\n",
    "  idx = 0\n",
    "  i = 0\n",
    "  for param in model.parameters():\n",
    "    delta = np.product(model_shapes[i])\n",
    "    block = flat_param[idx:idx+delta]\n",
    "    block = np.reshape(block, model_shapes[i])\n",
    "    i += 1\n",
    "    idx += delta\n",
    "    block_data = torch.from_numpy(block).float()\n",
    "    if args.cuda:\n",
    "      block_data = block_data.cuda()\n",
    "    param.data = block_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, print_mode=True, return_loss=False):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  for data, target in test_loader:\n",
    "    if args.cuda:\n",
    "      data, target = data.cuda(), target.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "    pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  acc = correct / len(test_loader.dataset)\n",
    "  \n",
    "  if print_mode:\n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
    "      test_loss, correct, len(test_loader.dataset),\n",
    "      100. * acc))\n",
    "  \n",
    "  if return_loss:\n",
    "    return test_loss\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50_w,101)-aCMA-ES (mu_w=27.2,w_1=8%) in dimension 11274 (seed=866159, Tue Oct 19 19:38:27 2021)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "es = SimpleES(NPARAMS,\n",
    "              popsize=NPOPULATION,\n",
    "              sigma_init=0.01,\n",
    "              sigma_decay=0.999,\n",
    "              sigma_alpha=0.2,\n",
    "              sigma_limit=0.001,\n",
    "              elite_ratio=0.1,\n",
    "              average_baseline=False,\n",
    "              forget_best=True\n",
    "             )\n",
    "\"\"\"\n",
    "es = CMAES(NPARAMS, sigma_init=0.01, popsize=NPOPULATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(procnum, model, solution, data, target, send_end):\n",
    "  update_model(solution, model, model_shapes)\n",
    "  output = model(data)\n",
    "  loss = F.nll_loss(output, target)\n",
    "  reward = - loss.data[0]\n",
    "  send_end.send(reward)\n",
    "\n",
    "def batch_simulation(model_list, solutions, data, target, process_count):\n",
    "  jobs = []\n",
    "  pipe_list = []\n",
    "\n",
    "  for i in range(process_count):\n",
    "    recv_end, send_end = mp.Pipe(False)\n",
    "    p = mp.Process(target=worker, args=(i, model_list[i], solutions[i], data, target, send_end))\n",
    "    jobs.append(p)\n",
    "    pipe_list.append(recv_end)\n",
    "\n",
    "  for p in jobs:\n",
    "    p.start()\n",
    "\n",
    "  for p in jobs:\n",
    "    p.join()\n",
    "\n",
    "  result_list = [x.recv() for x in pipe_list]\n",
    "  return np.array(result_list)\n",
    "\n",
    "\n",
    "def batch_simulation_sequential(model_list, solutions, data, target, process_count):\n",
    "  result_list = []\n",
    "  for i in range(process_count):\n",
    "    update_model(solutions[i], model_list[i], model_shapes)\n",
    "    output = model_list[i](data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    reward = - loss.item()\n",
    "    result_list.append(reward)\n",
    "  return np.array(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-aa6b7b8bbee1>:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 -0.06365200877189636 0.00026234898526122127 0.008148553549200275\n",
      "1 5 -0.11265005171298981 0.00026234898526122127 0.008147681191086059\n",
      "1 10 -0.0549115315079689 0.00026234898526122127 0.00814684324188775\n",
      "1 15 -0.033640455454587936 0.00026234898526122127 0.008145923466765974\n",
      "1 20 -0.05111776664853096 0.00026234898526122127 0.008145136113342178\n",
      "1 25 -0.05194903165102005 0.00026234898526122127 0.008144454359582536\n",
      "1 30 -0.0928168073296547 0.00026234898526122127 0.008143729743451464\n",
      "1 35 -0.09226616472005844 0.00026234898526122127 0.008143248093263895\n",
      "1 40 -0.07766052335500717 0.00026234898526122127 0.008142865162504294\n",
      "1 45 -0.06344421207904816 0.00026234898526122127 0.008142577814937763\n",
      "1 50 -0.05036438629031181 0.00026234898526122127 0.008142345628237334\n",
      "1 55 -0.03070640191435814 0.00026234898526122127 0.008141902926256099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-e1ba295ddbc0>:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_acc tensor(97.9533)\n",
      "best valid_acc tensor(97.9533)\n",
      "2 0 -0.057522114366292953 0.00026234898526122127 0.008141261816021624\n",
      "2 5 -0.08564811199903488 0.00026234898526122127 0.008140625643835128\n",
      "2 10 -0.05179399251937866 0.00026234898526122127 0.008140153303507938\n",
      "2 15 -0.04437222704291344 0.00026234898526122127 0.008139662655340297\n",
      "2 20 -0.06034574657678604 0.00026234898526122127 0.008139071064770933\n",
      "2 25 -0.048703182488679886 0.00026234898526122127 0.008138438405723225\n",
      "2 30 -0.03990330174565315 0.00026234898526122127 0.008137864345896577\n",
      "2 35 -0.06284601986408234 0.00026234898526122127 0.008137322033247388\n",
      "2 40 -0.05604536458849907 0.00026234898526122127 0.008136851574423438\n",
      "2 45 -0.0775589719414711 0.00026234898526122127 0.00813622908730436\n",
      "2 50 -0.07662402838468552 0.00026234898526122127 0.008135669564543986\n",
      "2 55 -0.06286252290010452 0.00026234898526122127 0.008135283100464795\n",
      "valid_acc tensor(97.8817)\n",
      "3 0 -0.09410808980464935 0.00026234898526122127 0.008135079183306769\n",
      "3 5 -0.04823760688304901 0.00026234898526122127 0.008135162518985588\n",
      "3 10 -0.05453388765454292 0.00026234898526122127 0.008135202528621962\n",
      "3 15 -0.04458298161625862 0.00026234898526122127 0.00813523183368509\n",
      "3 20 -0.06501515954732895 0.00026234898526122127 0.008135039719505974\n",
      "3 25 -0.06139690428972244 0.00026234898526122127 0.008134799268505135\n",
      "3 30 -0.06452712416648865 0.00026234898526122127 0.008134586927819687\n",
      "3 35 -0.05345245450735092 0.00026234898526122127 0.008134507890351634\n",
      "3 40 -0.056195057928562164 0.00026234898526122127 0.008134485033592738\n",
      "3 45 -0.051442671567201614 0.00026234898526122127 0.008134223555545703\n",
      "3 50 -0.07550369203090668 0.00026234898526122127 0.008134155836262315\n",
      "3 55 -0.06523726880550385 0.00026234898526122127 0.008134250207538163\n",
      "valid_acc tensor(97.9467)\n",
      "4 0 -0.09648510813713074 0.00026234898526122127 0.008134416225821546\n",
      "4 5 -0.07556292414665222 0.00026234898526122127 0.008134474281895352\n",
      "4 10 -0.05659247934818268 0.00026234898526122127 0.00813463880101884\n",
      "4 15 -0.06670043617486954 0.00026234898526122127 0.008134606105458943\n",
      "4 20 -0.061570897698402405 0.00026234898526122127 0.008134409349998904\n",
      "4 25 -0.0676213949918747 0.00026234898526122127 0.008134140306036418\n",
      "4 30 -0.05248689278960228 0.00026234898526122127 0.008133924822628128\n",
      "4 35 -0.057609811425209045 0.00026234898526122127 0.008133968189686593\n",
      "4 40 -0.08170783519744873 0.00026234898526122127 0.008133913297267245\n",
      "4 45 -0.0541619136929512 0.00026234898526122127 0.00813382764793563\n",
      "4 50 -0.045488860458135605 0.00026234898526122127 0.008133677162581152\n",
      "4 55 -0.06618517637252808 0.00026234898526122127 0.008133331365053419\n",
      "valid_acc tensor(98.0283)\n",
      "best valid_acc tensor(98.0283)\n",
      "5 0 -0.06596805900335312 0.00026234898526122127 0.00813298866797352\n",
      "5 5 -0.06025971472263336 0.00026234898526122127 0.008132682983798656\n",
      "5 10 -0.06420913338661194 0.00026234898526122127 0.008132383470968273\n",
      "5 15 -0.03801041096448898 0.00026234898526122127 0.008132046073329634\n",
      "5 20 -0.07733715325593948 0.00026234898526122127 0.008131704219326533\n",
      "5 25 -0.046848226338624954 0.00026234898526122127 0.008131250444747279\n",
      "5 30 -0.07181042432785034 0.00026234898526122127 0.008130728739562894\n",
      "5 35 -0.0648987889289856 0.00026234898526122127 0.00813016819532863\n",
      "5 40 -0.062402237206697464 0.00026234898526122127 0.0081294083090552\n",
      "5 45 -0.06091178208589554 0.00026234898526122127 0.0081285429788728\n",
      "5 50 -0.09061482548713684 0.00026234898526122127 0.008127686706407901\n",
      "5 55 -0.07005274295806885 0.00026234898526122127 0.008126990958297612\n",
      "valid_acc tensor(97.9967)\n",
      "6 0 -0.06376264244318008 0.00026234898526122127 0.008126268414689613\n",
      "6 5 -0.05887838080525398 0.00026234898526122127 0.008125640102947196\n",
      "6 10 -0.03679383546113968 0.00026234898526122127 0.008125138763214516\n",
      "6 15 -0.0794176459312439 0.00026234898526122127 0.008124605293656195\n",
      "6 20 -0.08143620938062668 0.00026234898526122127 0.008124009207391944\n",
      "6 25 -0.048520348966121674 0.00026234898526122127 0.008123398151939567\n",
      "6 30 -0.07409695535898209 0.00026234898526122127 0.00812289839890515\n",
      "6 35 -0.0697908028960228 0.00026234898526122127 0.008122494281142627\n",
      "6 40 -0.06435315310955048 0.00026234898526122127 0.008122152285890711\n",
      "6 45 -0.05739663913846016 0.00026234898526122127 0.008121445868567015\n",
      "6 50 -0.06057720258831978 0.00026234898526122127 0.008120565431812082\n",
      "6 55 -0.06320399045944214 0.00026234898526122127 0.00811983938817083\n",
      "valid_acc tensor(97.9833)\n",
      "7 0 -0.05121868848800659 0.00026234898526122127 0.00811917694667617\n",
      "7 5 -0.06351619213819504 0.00026234898526122127 0.008118528602975355\n",
      "7 10 -0.07818979769945145 0.00026234898526122127 0.008117766510855069\n",
      "7 15 -0.049503810703754425 0.00026234898526122127 0.0081167372817259\n",
      "7 20 -0.05952565371990204 0.00026234898526122127 0.00811556676470608\n",
      "7 25 -0.04600066691637039 0.00026234898526122127 0.008114226548064728\n",
      "7 30 -0.075363889336586 0.00026234898526122127 0.008112987195321978\n",
      "7 35 -0.07441465556621552 0.00026234898526122127 0.00811180712490757\n",
      "7 40 -0.07364482432603836 0.00026234898526122127 0.008110765253376583\n",
      "7 45 -0.04165758937597275 0.00026234898526122127 0.008109825428711852\n",
      "7 50 -0.03645410016179085 0.00026234898526122127 0.008108941564955162\n",
      "7 55 -0.06933894753456116 0.00026234898526122127 0.008108211134672567\n",
      "valid_acc tensor(98.0233)\n",
      "8 0 -0.043729811906814575 0.00026234898526122127 0.008107624340545213\n",
      "8 5 -0.036534834653139114 0.00026234898526122127 0.008106999546024687\n",
      "8 10 -0.05229409784078598 0.00026234898526122127 0.008106302225222989\n",
      "8 15 -0.08602898567914963 0.00026234898526122127 0.008105477456552504\n",
      "8 20 -0.057848010212183 0.00026234898526122127 0.008104431694149258\n",
      "8 25 -0.0590076707303524 0.00026234898526122127 0.008103598965630345\n",
      "8 30 -0.046907469630241394 0.00026234898526122127 0.008102881748621064\n",
      "8 35 -0.048997506499290466 0.00026234898526122127 0.00810213347894046\n",
      "8 40 -0.04753806069493294 0.00026234898526122127 0.00810126971321118\n",
      "8 45 -0.06250937283039093 0.00026234898526122127 0.00810043693358615\n",
      "8 50 -0.05110885575413704 0.00026234898526122127 0.008099717405550835\n",
      "8 55 -0.05218596011400223 0.00026234898526122127 0.008099094248406609\n",
      "valid_acc tensor(97.9717)\n",
      "9 0 -0.060109786689281464 0.00026234898526122127 0.008098414525032066\n",
      "9 5 -0.05748014152050018 0.00026234898526122127 0.008097526619283814\n",
      "9 10 -0.05393896624445915 0.00026234898526122127 0.008096501957083242\n",
      "9 15 -0.08804760128259659 0.00026234898526122127 0.008095502143316686\n",
      "9 20 -0.04509562999010086 0.00026234898526122127 0.008094669031599992\n",
      "9 25 -0.06856395304203033 0.00026234898526122127 0.008094105167763254\n",
      "9 30 -0.04527851194143295 0.00026234898526122127 0.008093451628157462\n",
      "9 35 -0.10046227276325226 0.00026234898526122127 0.008092665847994274\n",
      "9 40 -0.07122189551591873 0.00026234898526122127 0.008091767198386132\n",
      "9 45 -0.05694101005792618 0.00026234898526122127 0.008090840546702992\n",
      "9 50 -0.06408432126045227 0.00026234898526122127 0.008090033063025127\n",
      "9 55 -0.04429455101490021 0.00026234898526122127 0.008089338637008406\n",
      "valid_acc tensor(97.9800)\n",
      "10 0 -0.057324688881635666 0.00026234898526122127 0.008088517934162322\n",
      "10 5 -0.07966282963752747 0.00026234898526122127 0.008087548546124307\n",
      "10 10 -0.05369533598423004 0.00026234898526122127 0.008086818707561486\n",
      "10 15 -0.07921181619167328 0.00026234898526122127 0.008086102995034018\n",
      "10 20 -0.05311523750424385 0.00026234898526122127 0.00808538433933085\n",
      "10 25 -0.10119885206222534 0.00026234898526122127 0.008084834044202453\n",
      "10 30 -0.06957828253507614 0.00026234898526122127 0.008084484503946802\n",
      "10 35 -0.05102242901921272 0.00026234898526122127 0.008084016801938376\n",
      "10 40 -0.05970025807619095 0.00026234898526122127 0.00808347965091459\n",
      "10 45 -0.05805228278040886 0.00026234898526122127 0.008082871299264868\n",
      "10 50 -0.06754569709300995 0.00026234898526122127 0.008082359717324105\n",
      "10 55 -0.06288041174411774 0.00026234898526122127 0.00808167363515401\n",
      "valid_acc tensor(98.0433)\n",
      "best valid_acc tensor(98.0433)\n",
      "11 0 -0.05886336416006088 0.00026234898526122127 0.008080896993324711\n",
      "11 5 -0.06064232438802719 0.00026234898526122127 0.008080101787966117\n",
      "11 10 -0.04924805834889412 0.00026234898526122127 0.008079301859562042\n",
      "11 15 -0.04598138481378555 0.00026234898526122127 0.008078407187034946\n",
      "11 20 -0.07832455635070801 0.00026234898526122127 0.008077691980800019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 25 -0.056776586920022964 0.00026234898526122127 0.008077012829964888\n",
      "11 30 -0.08125486224889755 0.00026234898526122127 0.008076357712925296\n",
      "11 35 -0.06922829151153564 0.00026234898526122127 0.008075668089262607\n",
      "11 40 -0.07835672050714493 0.00026234898526122127 0.008074682701893437\n",
      "11 45 -0.055087972432374954 0.00026234898526122127 0.008073524598983226\n",
      "11 50 -0.07416422665119171 0.00026234898526122127 0.008072359213875509\n",
      "11 55 -0.04277100786566734 0.00026234898526122127 0.00807106582197097\n",
      "valid_acc tensor(97.9783)\n",
      "12 0 -0.06377795338630676 0.00026234898526122127 0.008069725136608136\n",
      "12 5 -0.06504996120929718 0.00026234898526122127 0.008068402418322558\n",
      "12 10 -0.0688546746969223 0.00026234898526122127 0.008066968066356407\n",
      "12 15 -0.0751860961318016 0.00026234898526122127 0.008065593869856152\n",
      "12 20 -0.07521370053291321 0.00026234898526122127 0.008064175317913551\n",
      "12 25 -0.07506944239139557 0.00026234898526122127 0.008062695789156358\n",
      "12 30 -0.07762980461120605 0.00026234898526122127 0.008060946692167233\n",
      "12 35 -0.04770863056182861 0.00026234898526122127 0.00805902420799155\n",
      "12 40 -0.03914973512291908 0.00026234898526122127 0.008057219962711026\n",
      "12 45 -0.06599928438663483 0.00026234898526122127 0.008055455427245662\n",
      "12 50 -0.07063315063714981 0.00026234898526122127 0.00805376990773695\n",
      "12 55 -0.05767779052257538 0.00026234898526122127 0.008052344328824405\n",
      "valid_acc tensor(98.0033)\n",
      "13 0 -0.05418844521045685 0.00026234898526122127 0.008050877273838918\n",
      "13 5 -0.0716501772403717 0.00026234898526122127 0.008049171616347108\n",
      "13 10 -0.060241565108299255 0.00026234898526122127 0.008047581597777175\n",
      "13 15 -0.08980958163738251 0.00026234898526122127 0.008046018668431503\n",
      "13 20 -0.04570610821247101 0.00026234898526122127 0.008044563415622204\n",
      "13 25 -0.07013599574565887 0.00026234898526122127 0.008043012057658235\n",
      "13 30 -0.06728635728359222 0.00026234898526122127 0.008041381007938244\n",
      "13 35 -0.06485242396593094 0.00026234898526122127 0.008039556387599915\n",
      "13 40 -0.06741458177566528 0.00026234898526122127 0.00803774683989331\n",
      "13 45 -0.054357826709747314 0.00026234898526122127 0.008035971694970001\n",
      "13 50 -0.09302397817373276 0.00026234898526122127 0.008034104533304484\n",
      "13 55 -0.06050699204206467 0.00026234898526122127 0.008032078214062613\n",
      "valid_acc tensor(97.9967)\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "best_valid_acc = 0\n",
    "training_log = []\n",
    "for epoch in range(1, args.epochs):\n",
    "\n",
    "  # train loop\n",
    "  model.eval()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    if args.cuda:\n",
    "      data, target = data.cuda(), target.cuda()\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    \n",
    "    solutions = es.ask()\n",
    "    reward = np.zeros(es.popsize)\n",
    "    \n",
    "    for i in range(es.popsize):\n",
    "      update_model(solutions[i], model, model_shapes)\n",
    "      output = model(data)\n",
    "      loss = F.nll_loss(output, target)\n",
    "      reward[i] = - loss.item()\n",
    "\n",
    "    best_raw_reward = reward.max()\n",
    "    #reward = compute_centered_ranks(reward)\n",
    "    #l2_decay = compute_weight_decay(weight_decay_coef, solutions)\n",
    "    #reward += l2_decay\n",
    "\n",
    "    es.tell(reward)\n",
    "\n",
    "    result = es.result()\n",
    "    \n",
    "    if (batch_idx % 5 == 0):\n",
    "      print(epoch, batch_idx, best_raw_reward, result[0].mean(), result[3].mean())\n",
    "\n",
    "  curr_solution = es.current_param()\n",
    "  update_model(curr_solution, model, model_shapes)\n",
    "\n",
    "  valid_acc = evaluate(model, valid_loader, print_mode=False)\n",
    "  training_log.append([epoch, valid_acc])\n",
    "  print('valid_acc', valid_acc * 100.)\n",
    "  if valid_acc >= best_valid_acc:\n",
    "    best_valid_acc = valid_acc\n",
    "    best_model = copy.deepcopy(model)\n",
    "    print('best valid_acc', best_valid_acc * 100.)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_model, valid_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_model, test_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_model, train_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model(es.best_param(), model, model_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, valid_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model(es.current_param(), model, model_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, valid_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, train_loader, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = evaluate(best_model, test_loader)\n",
    "print('final test acc', eval_acc * 100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_count = 0\n",
    "for param in model.parameters():\n",
    "  print(param.data.shape)\n",
    "  param_count += np.product(param.data.shape)\n",
    "print(param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_params = []\n",
    "for param in orig_model.parameters():\n",
    "  orig_params.append(param.data.cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_params_flat = np.concatenate(orig_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(orig_params_flat, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = []\n",
    "for param in best_model.parameters():\n",
    "  final_params.append(param.data.cpu().numpy().flatten())\n",
    "final_params_flat = np.concatenate(final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(final_params_flat, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
