{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cma in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting BindsNET@ git+https://github.com/BindsNET/bindsnet.git@ead55217e05ba4c6ef27f45ff5dc7d61b4abaa13\n",
      "  Using cached BindsNET-0.2.9-py3-none-any.whl\n",
      "Collecting gym@ git+https://github.com/openai/gym.git@a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022\n",
      "  Using cached gym-0.18.0-py3-none-any.whl\n",
      "Requirement already satisfied: appdirs==1.4.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: cma in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: atari-py==0.2.6 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: attrs==20.3.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 4)) (20.3.0)\n",
      "Requirement already satisfied: box2d-py==2.3.8 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 6)) (2.3.8)\n",
      "Requirement already satisfied: cffi==1.14.5 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 7)) (1.14.5)\n",
      "Requirement already satisfied: cloudpickle==1.2.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 8)) (1.2.2)\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 10)) (0.29.23)\n",
      "Requirement already satisfied: decorator==4.4.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 11)) (4.4.2)\n",
      "Requirement already satisfied: distlib==0.3.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 12)) (0.3.1)\n",
      "Requirement already satisfied: distro==1.5.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 13)) (1.5.0)\n",
      "Requirement already satisfied: fasteners==0.16 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 14)) (0.16)\n",
      "Requirement already satisfied: filelock==3.0.12 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 15)) (3.0.12)\n",
      "Requirement already satisfied: glfw==2.1.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 16)) (2.1.0)\n",
      "Requirement already satisfied: imageio==2.9.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 18)) (2.9.0)\n",
      "Requirement already satisfied: iniconfig==1.1.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 19)) (1.1.1)\n",
      "Requirement already satisfied: joblib==1.0.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 20)) (1.0.1)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 21)) (1.3.1)\n",
      "Requirement already satisfied: matplotlib==3.4.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 22)) (3.4.1)\n",
      "Requirement already satisfied: mujoco-py==2.0.2.13 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 23)) (2.0.2.13)\n",
      "Requirement already satisfied: networkx==2.5.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 24)) (2.5.1)\n",
      "Requirement already satisfied: numpy==1.20.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 25)) (1.20.2)\n",
      "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 26)) (4.5.1.48)\n",
      "Requirement already satisfied: packaging==20.9 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 27)) (20.9)\n",
      "Requirement already satisfied: pandas==1.2.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 28)) (1.2.4)\n",
      "Requirement already satisfied: pbr==5.6.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 29)) (5.6.0)\n",
      "Requirement already satisfied: Pillow==8.2.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 30)) (8.2.0)\n",
      "Requirement already satisfied: pluggy==0.13.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 31)) (0.13.1)\n",
      "Requirement already satisfied: protobuf==3.15.8 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 32)) (3.15.8)\n",
      "Requirement already satisfied: py==1.10.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 33)) (1.10.0)\n",
      "Requirement already satisfied: pybullet==3.1.6 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 34)) (3.1.6)\n",
      "Requirement already satisfied: pycparser==2.20 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 35)) (2.20)\n",
      "Requirement already satisfied: pyglet==1.5.11 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 36)) (1.5.11)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 37)) (2.4.7)\n",
      "Requirement already satisfied: pytest==6.2.3 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 38)) (6.2.3)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 39)) (2.8.1)\n",
      "Requirement already satisfied: pytz==2021.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 40)) (2021.1)\n",
      "Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 41)) (1.1.1)\n",
      "Requirement already satisfied: scikit-build==0.11.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 42)) (0.11.1)\n",
      "Requirement already satisfied: scikit-image==0.18.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 43)) (0.18.1)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 44)) (0.24.1)\n",
      "Requirement already satisfied: scipy==1.6.3 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 45)) (1.6.3)\n",
      "Requirement already satisfied: six==1.15.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 46)) (1.15.0)\n",
      "Requirement already satisfied: stevedore==3.3.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 47)) (3.3.0)\n",
      "Requirement already satisfied: tensorboardX==2.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 48)) (2.2)\n",
      "Requirement already satisfied: threadpoolctl==2.1.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 49)) (2.1.0)\n",
      "Requirement already satisfied: tifffile==2021.4.8 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 50)) (2021.4.8)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 51)) (0.10.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 52)) (1.8.1)\n",
      "Requirement already satisfied: torchvision==0.9.1 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 53)) (0.9.1)\n",
      "Requirement already satisfied: tqdm==4.60.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 54)) (4.60.0)\n",
      "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 55)) (3.7.4.3)\n",
      "Requirement already satisfied: virtualenv==20.4.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 56)) (20.4.4)\n",
      "Requirement already satisfied: virtualenv-clone==0.5.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 57)) (0.5.4)\n",
      "Requirement already satisfied: virtualenvwrapper==4.8.4 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from -r ../../requirements.txt (line 58)) (4.8.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools>=28.0.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from scikit-build==0.11.1->-r ../../requirements.txt (line 42)) (54.2.0)\n",
      "Requirement already satisfied: wheel>=0.29.0 in /usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages (from scikit-build==0.11.1->-r ../../requirements.txt (line 42)) (0.36.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/Cellar/jupyterlab/3.0.14/libexec/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cma\n",
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import cma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import codecs\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "from statistics import mean\n",
    "\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection, Conv2dConnection, MaxPool2dConnection\n",
    "from bindsnet.network.monitors import Monitor\n",
    "from bindsnet.network.nodes import AbstractInput\n",
    "from bindsnet.learning import PostPre, WeightDependentPostPre, Hebbian, MSTDP, MSTDPET\n",
    "\n",
    "from bindsnet.environment import GymEnvironment\n",
    "from torch import Tensor\n",
    "from bindsnet.analysis.plotting import plot_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count() 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "print(\"torch.cuda.device_count()\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', ['epochs', 'batch_size', 'lr', 'cuda', 'seed', 'log_interval', 'env', 'fitness_episodes', 'gamma', 'n_episode', 'update_rule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(batch_size=1000, epochs=30, lr=0.001, cuda=False, seed=0, log_interval=10, gamma=0.99, env='BreakoutDeterministic-v4', fitness_episodes=5, n_episode=1000, update_rule='MSTDP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = {\n",
    "    \"PostPre\": PostPre,\n",
    "    \"WeightDependentPostPre\": WeightDependentPostPre,\n",
    "    \"Hebbian\": Hebbian,\n",
    "    \"MSTDP\": MSTDP,\n",
    "    \"MSTDPET\": MSTDPET,\n",
    "}\n",
    "s_im, s_ax = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(x):\n",
    "  \"\"\"\n",
    "  Returns ranks in [0, len(x))\n",
    "  Note: This is different from scipy.stats.rankdata, which returns ranks in [1, len(x)].\n",
    "  (https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py)\n",
    "  \"\"\"\n",
    "  assert x.ndim == 1\n",
    "  ranks = np.empty(len(x), dtype=int)\n",
    "  ranks[x.argsort()] = np.arange(len(x))\n",
    "  return ranks\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "  \"\"\"\n",
    "  https://github.com/openai/evolution-strategies-starter/blob/master/es_distributed/es.py\n",
    "  \"\"\"\n",
    "  y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "  y /= (x.size - 1)\n",
    "  y -= .5\n",
    "  return y\n",
    "\n",
    "def compute_weight_decay(weight_decay, model_param_list):\n",
    "  model_param_grid = np.array(model_param_list)\n",
    "  return - weight_decay * np.mean(model_param_grid * model_param_grid, axis=1)\n",
    "\n",
    "class CMAES:\n",
    "  '''CMA-ES wrapper.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.10,       # initial standard deviation\n",
    "               popsize=255):          # population size\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.popsize = popsize\n",
    "\n",
    "    self.solutions = None\n",
    "\n",
    "    import cma\n",
    "    self.es = cma.CMAEvolutionStrategy( self.num_params * [0],\n",
    "                                        self.sigma_init,\n",
    "                                        {'popsize': self.popsize})\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.es.result[6]\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    self.solutions = np.array(self.es.ask())\n",
    "    return self.solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    reward_table = reward_table_result\n",
    "    self.es.tell(self.solutions, (-reward_table).tolist()) # convert minimizer to maximizer.\n",
    "\n",
    "  def done(self):\n",
    "    return self.es.stop()\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.es.result[5] # mean solution, presumably better with noise\n",
    "  \n",
    "  def best_param(self):\n",
    "    return self.es.result[0] # best evaluated solution\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    r = self.es.result\n",
    "    return (r[0], -r[1], -r[1], r[6])\n",
    "\n",
    "class SimpleES:\n",
    "  '''Simple Evolution Strategies.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.10,       # initial standard deviation\n",
    "               sigma_alpha=0.20,      # learning rate for standard deviation\n",
    "               sigma_decay=0.999,     # anneal standard deviation\n",
    "               sigma_limit=0.01,      # stop annealing if less than this\n",
    "               popsize=255,           # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               done_threshold=1e-6,   # threshold when we say we are done\n",
    "               average_baseline=True, # set baseline to average of batch\n",
    "               forget_best=True):     # only use the best from latest generation\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.sigma_alpha = sigma_alpha\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.popsize = popsize\n",
    "    self.average_baseline = average_baseline\n",
    "    if self.average_baseline:\n",
    "      assert (self.popsize & 2), \"Population size must be even\"\n",
    "      self.batch_size = int(self.popsize / 2)\n",
    "    else:\n",
    "      assert (self.popsize & 1), \"Population size must be odd\"\n",
    "      self.batch_size = int((self.popsize - 1) / 2)\n",
    "    self.elite_ratio = elite_ratio\n",
    "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
    "    self.forget_best = forget_best\n",
    "    self.batch_reward = np.zeros(self.batch_size * 2)\n",
    "    self.mu = np.zeros(self.num_params)\n",
    "    self.sigma = np.ones(self.num_params) * self.sigma_init\n",
    "    self.curr_best_mu = np.zeros(self.num_params)\n",
    "    self.best_mu = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_interation = True\n",
    "    self.done_threshold = done_threshold\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.sigma\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    self.epsilon = np.random.randn(self.batch_size, self.num_params) * self.sigma.reshape(1, self.num_params)\n",
    "    self.epsilon_full = np.concatenate([self.epsilon, - self.epsilon])\n",
    "    if self.average_baseline:\n",
    "      epsilon = self.epsilon_full\n",
    "    else:\n",
    "      # first population is mu, then positive epsilon, then negative epsilon\n",
    "      epsilon = np.concatenate([np.zeros((1, self.num_params)), self.epsilon_full])\n",
    "    solutions = self.mu.reshape(1, self.num_params) + epsilon\n",
    "    return solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "\n",
    "    reward_table = reward_table_result\n",
    "\n",
    "    reward_offset = 1\n",
    "    if self.average_baseline:\n",
    "      b = np.mean(reward_table)\n",
    "      reward_offset = 0\n",
    "    else:\n",
    "      b = reward_table[0] # baseline\n",
    "      \n",
    "    reward = reward_table[reward_offset:]\n",
    "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
    "\n",
    "    best_reward = reward[idx[0]]\n",
    "    if (best_reward > b or self.average_baseline):\n",
    "      best_mu = self.mu + self.epsilon_full[idx[0]]\n",
    "      best_reward = reward[idx[0]]\n",
    "    else:\n",
    "      best_mu = self.mu\n",
    "      best_reward = b\n",
    "\n",
    "    self.curr_best_reward = best_reward\n",
    "    self.curr_best_mu = best_mu\n",
    "\n",
    "    if self.first_interation:\n",
    "      self.first_interation = False\n",
    "      self.best_reward = self.curr_best_reward\n",
    "      self.best_mu = best_mu\n",
    "    else:\n",
    "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "        self.best_mu = best_mu\n",
    "        self.best_reward = self.curr_best_reward\n",
    "\n",
    "    # adaptive sigma\n",
    "    # normalization\n",
    "    stdev_reward = reward.std()\n",
    "    epsilon = self.epsilon\n",
    "    sigma = self.sigma\n",
    "    S = ((epsilon * epsilon - (sigma * sigma).reshape(1, self.num_params)) / sigma.reshape(1, self.num_params))\n",
    "    reward_avg = (reward[:self.batch_size] + reward[self.batch_size:]) / 2.0\n",
    "    rS = reward_avg - b\n",
    "    delta_sigma = (np.dot(rS, S)) / (2 * self.batch_size * stdev_reward)\n",
    "\n",
    "    # move mean to the average of the best idx means\n",
    "    self.mu += self.epsilon_full[idx].mean(axis=0)\n",
    "\n",
    "    # adjust sigma according to the adaptive sigma calculation\n",
    "    change_sigma = self.sigma_alpha * delta_sigma\n",
    "    change_sigma = np.minimum(change_sigma, self.sigma)\n",
    "    change_sigma = np.maximum(change_sigma, - 0.5 * self.sigma)\n",
    "    self.sigma += change_sigma\n",
    "    self.sigma[self.sigma > self.sigma_limit] *= self.sigma_decay\n",
    "\n",
    "  def done(self):\n",
    "    return (self.rms_stdev() < self.done_threshold)\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.curr_best_mu\n",
    "  \n",
    "  def best_param(self):\n",
    "    return self.best_mu\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)\n",
    "\n",
    "class SimpleGA:\n",
    "  '''Simple Genetic Algorithm.'''\n",
    "  def __init__(self, num_params,      # number of model parameters\n",
    "               sigma_init=0.1,        # initial standard deviation\n",
    "               sigma_decay=0.999,     # anneal standard deviation\n",
    "               sigma_limit=0.01,      # stop annealing if less than this\n",
    "               popsize=255,           # population size\n",
    "               elite_ratio=0.1,       # percentage of the elites\n",
    "               forget_best=False,     # forget the historical best elites\n",
    "               done_threshold=1e-6):  # threshold when we say we are done\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_init = sigma_init\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.popsize = popsize\n",
    "\n",
    "    self.elite_ratio = elite_ratio\n",
    "    self.elite_popsize = int(self.popsize * self.elite_ratio)\n",
    "\n",
    "    self.sigma = self.sigma_init\n",
    "    self.elite_params = np.zeros((self.elite_popsize, self.num_params))\n",
    "    self.elite_rewards = np.zeros(self.elite_popsize)\n",
    "    self.best_param = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_iteration = True\n",
    "    self.forget_best = forget_best\n",
    "    self.done_threshold = done_threshold\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    return self.sigma # same sigma for all parameters.\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    self.epsilon = np.random.randn(self.popsize, self.num_params) * self.sigma\n",
    "    solutions = []\n",
    "    \n",
    "    def mate(a, b):\n",
    "      c = np.copy(a)\n",
    "      idx = np.where(np.random.rand((c.size)) > 0.5)\n",
    "      c[idx] = b[idx]\n",
    "      return c\n",
    "    \n",
    "    elite_range = range(self.elite_popsize)\n",
    "    for i in range(self.popsize):\n",
    "      idx_a = np.random.choice(elite_range)\n",
    "      idx_b = np.random.choice(elite_range)\n",
    "      child_params = mate(self.elite_params[idx_a], self.elite_params[idx_b])\n",
    "      solutions.append(child_params + self.epsilon[i])\n",
    "\n",
    "    solutions = np.array(solutions)\n",
    "    self.solutions = solutions\n",
    "\n",
    "    return solutions\n",
    "\n",
    "  def tell(self, reward_table_result):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward_table_result) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "    \n",
    "    if (not self.forget_best or self.first_iteration):\n",
    "      reward = reward_table_result\n",
    "      solution = self.solutions\n",
    "    else:\n",
    "      reward = np.concatenate([reward_table_result, self.elite_rewards])\n",
    "      solution = np.concatenate([self.solutions, self.elite_params])\n",
    "\n",
    "    idx = np.argsort(reward)[::-1][0:self.elite_popsize]\n",
    "\n",
    "    self.elite_rewards = reward[idx]\n",
    "    self.elite_params = solution[idx]\n",
    "\n",
    "    self.curr_best_reward = self.elite_rewards[0]\n",
    "    \n",
    "    if self.first_iteration or (self.curr_best_reward > self.best_reward):\n",
    "      self.first_iteration = False\n",
    "      self.best_reward = self.elite_rewards[0]\n",
    "      self.best_param = np.copy(self.elite_params[0])\n",
    "\n",
    "    if (self.sigma > self.sigma_limit):\n",
    "      self.sigma *= self.sigma_decay\n",
    "\n",
    "  def done(self):\n",
    "    return (self.rms_stdev() < self.done_threshold)\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.elite_params[0]\n",
    "\n",
    "  def best_param(self):\n",
    "    return self.best_param\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_param, self.best_reward, self.curr_best_reward, self.sigma)\n",
    "\n",
    "class OpenES:\n",
    "  ''' Basic Version of OpenAI Evolution Strategies.'''\n",
    "  def __init__(self, num_params,             # number of model parameters\n",
    "               sigma_init=0.1,               # initial standard deviation\n",
    "               sigma_decay=0.999,            # anneal standard deviation\n",
    "               sigma_limit=0.01,             # stop annealing if less than this\n",
    "               learning_rate=0.001,          # learning rate for standard deviation\n",
    "               learning_rate_decay = 0.9999, # annealing the learning rate\n",
    "               learning_rate_limit = 0.001,  # stop annealing learning rate\n",
    "               popsize=255,                  # population size\n",
    "               antithetic=False,             # whether to use antithetic sampling\n",
    "               forget_best=True):           # forget historical best\n",
    "\n",
    "    self.num_params = num_params\n",
    "    self.sigma_decay = sigma_decay\n",
    "    self.sigma = sigma_init\n",
    "    self.sigma_limit = sigma_limit\n",
    "    self.learning_rate = learning_rate\n",
    "    self.learning_rate_decay = learning_rate_decay\n",
    "    self.learning_rate_limit = learning_rate_limit\n",
    "    self.popsize = popsize\n",
    "    self.antithetic = antithetic\n",
    "    if self.antithetic:\n",
    "      assert (self.popsize & 2), \"Population size must be even\"\n",
    "      self.half_popsize = int(self.popsize / 2)\n",
    "\n",
    "    self.reward = np.zeros(self.popsize)\n",
    "    self.mu = np.zeros(self.num_params)\n",
    "    self.best_mu = np.zeros(self.num_params)\n",
    "    self.best_reward = 0\n",
    "    self.first_interation = True\n",
    "    self.forget_best = forget_best\n",
    "\n",
    "  def rms_stdev(self):\n",
    "    sigma = self.sigma\n",
    "    return np.mean(np.sqrt(sigma*sigma))\n",
    "\n",
    "  def ask(self):\n",
    "    '''returns a list of parameters'''\n",
    "    # antithetic sampling\n",
    "    if self.antithetic:\n",
    "      self.epsilon_half = np.random.randn(self.half_popsize, self.num_params)\n",
    "      self.epsilon = np.concatenate([self.epsilon_half, - self.epsilon_half])\n",
    "    else:\n",
    "      self.epsilon = np.random.randn(self.popsize, self.num_params)\n",
    "\n",
    "    self.solutions = self.mu.reshape(1, self.num_params) + self.epsilon * self.sigma\n",
    "\n",
    "    return self.solutions\n",
    "\n",
    "  def tell(self, reward):\n",
    "    # input must be a numpy float array\n",
    "    assert(len(reward) == self.popsize), \"Inconsistent reward_table size reported.\"\n",
    "\n",
    "    idx = np.argsort(reward)[::-1]\n",
    "\n",
    "    best_reward = reward[idx[0]]\n",
    "    best_mu = self.solutions[idx[0]]\n",
    "\n",
    "    self.curr_best_reward = best_reward\n",
    "    self.curr_best_mu = best_mu\n",
    "\n",
    "    if self.first_interation:\n",
    "      self.first_interation = False\n",
    "      self.best_reward = self.curr_best_reward\n",
    "      self.best_mu = best_mu\n",
    "    else:\n",
    "      if self.forget_best or (self.curr_best_reward > self.best_reward):\n",
    "        self.best_mu = best_mu\n",
    "        self.best_reward = self.curr_best_reward\n",
    "\n",
    "    # main bit:\n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    normalized_reward = (reward - np.mean(reward)) / np.std(reward)\n",
    "    self.mu += self.learning_rate/(self.popsize*self.sigma)*np.dot(self.epsilon.T, normalized_reward)\n",
    "\n",
    "    # adjust sigma according to the adaptive sigma calculation\n",
    "    if (self.sigma > self.sigma_limit):\n",
    "      self.sigma *= self.sigma_decay\n",
    "\n",
    "    if (self.learning_rate > self.learning_rate_limit):\n",
    "      self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "  def done(self):\n",
    "    return False\n",
    "\n",
    "  def current_param(self):\n",
    "    return self.curr_best_mu\n",
    "\n",
    "  def best_param(self):\n",
    "    return self.best_mu\n",
    "\n",
    "  def result(self): # return best params so far, along with historically best reward, curr reward, sigma\n",
    "    return (self.best_mu, self.best_reward, self.curr_best_reward, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"SQN Network\n",
    "        \"\"\"\n",
    "        input_dim, shape = 80 * 80, [1, 1, 80, 80]\n",
    "        output_dim = 4\n",
    "        hidden_dim = 10\n",
    "        \n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.network = Network(dt=1.0)\n",
    "        self.learning_rule = rules.get(args.update_rule)\n",
    "        self.time = int(self.network.dt)\n",
    "\n",
    "        # To solve tensor formatting issues\n",
    "        if args.update_rule == 'MSTDP' or args.update_rule == \"HMSTDP\":\n",
    "            self.input = Input(n=input_dim, traces=True)\n",
    "        else:\n",
    "            self.input = Input(n=input_dim, shape=shape, traces=True)\n",
    "\n",
    "        # Heterogeneous LIFNodes\n",
    "#         self.hidden = LIFNodes(n=hidden_dim, traces=True)\n",
    "        self.output = LIFNodes(n=output_dim, traces=True)\n",
    "\n",
    "        # First connection\n",
    "#         self.connection_input_hidden = Connection(\n",
    "#             source=self.input,\n",
    "#             target=self.hidden,\n",
    "#             update_rule=self.learning_rule,\n",
    "#             wmin=0,\n",
    "#             wmax=1,\n",
    "#             nu=args.gamma\n",
    "#         )\n",
    "\n",
    "        # Hidden recurrent connection\n",
    "#         self.connection_hidden_hidden = Connection(\n",
    "#             source=self.hidden,\n",
    "#             target=self.hidden,\n",
    "#             update_rule=self.learning_rule,\n",
    "#             wmin=0,\n",
    "#             wmax=1,\n",
    "#             nu=args.gamma\n",
    "#         )\n",
    "\n",
    "        # Hidden layer to Output\n",
    "#         self.connection_hidden_output = Connection(\n",
    "#             source=self.hidden,\n",
    "#             target=self.output,\n",
    "#             update_rule=self.learning_rule,\n",
    "#             wmin=0,\n",
    "#             wmax=1,\n",
    "#             nu=args.gamma\n",
    "#         )\n",
    "        self.connection_input_output = Connection(\n",
    "            source=self.input,\n",
    "            target=self.output,\n",
    "            update_rule=self.learning_rule,\n",
    "            wmin=-1,\n",
    "            wmax=1,\n",
    "            nu=args.gamma\n",
    "        )\n",
    "\n",
    "        self.network.add_layer(\n",
    "            layer=self.input, name=\"Input\"\n",
    "        )\n",
    "#         self.network.add_layer(\n",
    "#             layer=self.hidden, name=\"Hidden\"\n",
    "#         )\n",
    "        self.network.add_layer(\n",
    "            layer=self.output, name=\"Output\"\n",
    "        )\n",
    "    \n",
    "        self.network.add_connection(connection=self.connection_input_output, source=\"Input\", target=\"Output\")\n",
    "\n",
    "#         self.network.add_connection(\n",
    "#             connection=self.connection_input_hidden,\n",
    "#             source=\"Input\",\n",
    "#             target=\"Hidden\"\n",
    "#         )\n",
    "#         self.network.add_connection(\n",
    "#             connection=self.connection_hidden_hidden,\n",
    "#             source=\"Hidden\",\n",
    "#             target=\"Hidden\"\n",
    "#         )\n",
    "#         self.network.add_connection(\n",
    "#             connection=self.connection_hidden_output,\n",
    "#             source=\"Hidden\",\n",
    "#             target=\"Output\"\n",
    "#         )\n",
    "\n",
    "        self.inputs = [\n",
    "            name\n",
    "            for name, layer in self.network.layers.items()\n",
    "            if isinstance(layer, AbstractInput)\n",
    "        ]\n",
    "\n",
    "        # To record outputs\n",
    "        self.network.add_monitor(\n",
    "            Monitor(self.output, [\"s\"], time=self.time),\n",
    "            name=\"output_monitor\"\n",
    "        )\n",
    "#         self.network.add_monitor(\n",
    "#             Monitor(self.hidden, [\"s\"], time=self.time),\n",
    "#             name=\"hidden_monitor\"\n",
    "#         )\n",
    "        self.network.add_monitor(\n",
    "            Monitor(self.input, [\"s\"], time=self.time),\n",
    "            name=\"input_monitor\"\n",
    "        )\n",
    "\n",
    "        self.spike_record = {\n",
    "            \"Output\": torch.zeros((self.time, output_dim)).to(\n",
    "                self.device\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def run(self, inputs: dict[str, torch.Tensor], reward: [float, torch.Tensor], **kwargs) -> None:\n",
    "        self.network.train(mode=True)\n",
    "        return self.network.run(inputs=inputs, time=self.time, reward=reward, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        \"\"\"Agent class that chooses an action and trains\n",
    "        \"\"\"\n",
    "        self.net = model\n",
    "\n",
    "    def get_action(self) -> int:\n",
    "        \"\"\"Returns an action index\n",
    "        \"\"\"\n",
    "        scores = self.get_Q()\n",
    "#         probabilities = torch.softmax(scores, dim=0)\n",
    "#         return torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "        _, argmax = torch.max(torch.flatten(scores), dim=0)\n",
    "        return argmax.item()\n",
    "\n",
    "    def get_Q(self) -> Tensor:\n",
    "        \"\"\"Returns `Q-value` based on output layer's spikes\n",
    "        \"\"\"\n",
    "        return torch.sum(self.net.spike_record[\"Output\"], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_reward(reward):\n",
    "    \"\"\"Clip reward so that it's in [-1, 1]\n",
    "    \"\"\"\n",
    "    if reward < -1:\n",
    "        reward = -1\n",
    "    elif reward > 1:\n",
    "        reward = 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(states: np.ndarray):\n",
    "    \"\"\"Preprocesses gym state\n",
    "    \"\"\"\n",
    "    # Crop\n",
    "    states = states[34:194, 0:160, :]\n",
    "\n",
    "    # Convert to grayscale\n",
    "    states = cv2.cvtColor(states, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Subsample to 80x80\n",
    "    states = cv2.resize(states, (80, 80))\n",
    "    states = cv2.threshold(states, 0, 1, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    states = states.reshape(1, states.shape[0], states.shape[1])\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env: GymEnvironment, agent: Agent) -> int:\n",
    "    \"\"\"Play an epsiode and train\n",
    "    Args:\n",
    "        env (gym.Env): gym environment (CartPole-v0)\n",
    "        agent (Agent): agent will train and get action\n",
    "    Returns:\n",
    "        int: reward\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    agent.net.network.reset_state_variables()\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        # Select an action\n",
    "        a = agent.get_action()\n",
    "        # Update the state according to action a\n",
    "        s, r, done, info = env.step(a)\n",
    "\n",
    "        r = clip_reward(r)\n",
    "\n",
    "        # Tensor shape configuration\n",
    "        if args.update_rule == 'MSTDP' or args.update_rule == \"HMSTDP\":\n",
    "            s = s.flatten()\n",
    "\n",
    "        s_shape = [1] * len(s.shape[1:])\n",
    "\n",
    "        # Run the agent for time t on state s with reward r\n",
    "        inputs = {k: s.repeat(agent.net.time, *s_shape) for k in agent.net.inputs}\n",
    "        agent.net.run(inputs=inputs, reward=r)\n",
    "\n",
    "        # Update output spikes\n",
    "        if agent.net.output is not None:\n",
    "            agent.net.spike_record[\"Output\"] = (\n",
    "                agent.net.network.monitors[\"output_monitor\"].get(\"s\").float()\n",
    "            )\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "#         global s_im\n",
    "#         global s_ax\n",
    "#         s_im, s_ax = plot_spikes({\n",
    "#             \"input\": agent.net.network.monitors[\"input_monitor\"].get(\"s\"),\n",
    "#             \"hidden\": agent.net.network.monitors[\"hidden_monitor\"].get(\"s\"),\n",
    "#             \"output\": agent.net.network.monitors[\"output_monitor\"].get(\"s\")\n",
    "#         }, ims=s_im, axes=s_ax)\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_dim(env: gym.Env) -> Tuple[int, int]:\n",
    "    \"\"\"Returns input_dim & output_dim\n",
    "    Args:\n",
    "        env (gym.Env): gym Environment\n",
    "    Returns:\n",
    "        int: input_dim\n",
    "        int: output_dim\n",
    "    \"\"\"\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    return input_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPOPULATION = 50\n",
    "weight_decay_coef = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "models = []\n",
    "for i in range(NPOPULATION):\n",
    "  model = Net()\n",
    "  if args.cuda:\n",
    "    model.cuda()\n",
    "  model.eval()\n",
    "  models.append(model)\n",
    "'''\n",
    "\n",
    "model = Net()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "orig_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25600\n"
     ]
    }
   ],
   "source": [
    "# get init params\n",
    "orig_params = []\n",
    "model_shapes = []\n",
    "for param in orig_model.network.parameters():\n",
    "    p = param.data.cpu().numpy()\n",
    "    model_shapes.append(p.shape)\n",
    "    orig_params.append(p.flatten())\n",
    "orig_params_flat = np.concatenate(orig_params)\n",
    "NPARAMS = len(orig_params_flat)\n",
    "print(NPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(flat_param, model, model_shapes):\n",
    "    idx = 0\n",
    "    i = 0\n",
    "    for param in model.network.parameters():\n",
    "        delta = np.product(model_shapes[i])\n",
    "        block = flat_param[idx:idx+delta]\n",
    "        block = np.reshape(block, model_shapes[i])\n",
    "        i += 1\n",
    "        idx += delta\n",
    "        block_data = torch.from_numpy(block).float()\n",
    "        if args.cuda:\n",
    "          block_data = block_data.cuda()\n",
    "        param.data = block_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, print_mode=True):\n",
    "    agent = Agent(model)\n",
    "    total_reward = 0\n",
    "    for i in range(args.fitness_episodes):\n",
    "        reward = play_episode(env, agent)\n",
    "        total_reward += reward\n",
    "        \n",
    "    if print_mode:\n",
    "        print('\\nAverage reward: {:.4f}'.format(total_reward/args.fitness_episodes))\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25_w,50)-aCMA-ES (mu_w=14.0,w_1=14%) in dimension 25600 (seed=624334, Thu Oct 28 21:41:53 2021)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "es = SimpleES(NPARAMS,\n",
    "              popsize=NPOPULATION,\n",
    "              sigma_init=0.01,\n",
    "              sigma_decay=0.999,\n",
    "              sigma_alpha=0.2,\n",
    "              sigma_limit=0.001,\n",
    "              elite_ratio=0.1,\n",
    "              average_baseline=False,\n",
    "              forget_best=True\n",
    "             )\n",
    "\"\"\"\n",
    "es = CMAES(NPARAMS, sigma_init=0.01, popsize=NPOPULATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a997264a9e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolutions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbest_raw_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-7eddcbd94076>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, env, print_mode)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-1fa70dcdcab6>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Update the state according to action a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/bindsnet/environment/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Call gym's environment step function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_rewards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.14/libexec/lib/python3.9/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = GymEnvironment(args.env)\n",
    "try:\n",
    "    for epoch in range(1, args.epochs):\n",
    "\n",
    "        # train loop\n",
    "        model.network.eval()\n",
    "        solutions = es.ask()\n",
    "        reward = np.zeros(es.popsize)\n",
    "\n",
    "        for i in range(es.popsize):\n",
    "            update_model(solutions[i], model, model_shapes)\n",
    "            reward[i] = evaluate(model, env, print_mode=False)\n",
    "\n",
    "        best_raw_reward = reward.max()\n",
    "        es.tell(reward)\n",
    "        result = es.result()\n",
    "        print(\"\\n[Epoch: {:5}] Max average reward: {:.4f}\".format(epoch, best_raw_reward/args.fitness_episodes))\n",
    "\n",
    "        curr_solution = es.current_param()\n",
    "        update_model(curr_solution, model, model_shapes)\n",
    "        solution_reward = evaluate(model, env, print_mode=False)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
